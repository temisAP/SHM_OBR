import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np

class Compressor(nn.Module):
    """ Compressor/expansor for raw atributes (ss,cc,ac) """
    def __init__(self,input_dim,output_dim):
        super().__init__()
        # Dimensions
        self.input_dim = input_dim
        self.output_dim = output_dim
        N = 3
        dimensions = np.linspace(input_dim,output_dim,N+1,dtype = np.int32)
        # Layer secuence
        self.FC = nn.Sequential()
        for i in range(N):
            self.FC.add_module("Linear"+str(i),nn.Linear(dimensions[i],dimensions[i+1]))
            self.FC.add_module("BatchNorm"+str(i),nn.BatchNorm1d(dimensions[i+1]))
            self.FC.add_module("ReLu"+str(i),nn.ReLU())
        self.FC.add_module("Linear"+str(N),nn.Linear(dimensions[-1],output_dim))
    def forward(self,x):
        return self.FC(x)

class Convolutional(nn.Module):
    """ Takes 3 channels and returns 1 using a CNN (convolutional neural network)  """
    def __init__(self,input_chanels,output_chanels,output_dim):
        super().__init__()
        # Dimensions
        self.input_channels  = input_channels
        self.output_channels = output_channels
        self.output_dim = output_dim
        N = 2
        channels = np.linspace(input_chanels,output_chanels,N+1,dtype = np.int32)
        # Layer secuence
        self.CNN = nn.Sequential()
        for i in range(N-1):
            self.CNN.add_module("Conv"+str(i),nn.Conv1d(channels[i],chanels[i+1],kernel_size = 3,padding = 1))
            self.CNN.add_module("BatchNorm"+str(i),nn.BatchNorm1d(dimensions[i+1]))
            self.CNN.add_module("ReLu"+str(i),nn.ReLU())
        self.CNN.add_module("Flatten",nn.Flatten())
        self.CNN.add_module("Linear",nn.Linear(chanels[i+1],output_dim))
        self.CNN.add_module("Sigmoid",nn.Sigmoid())

        # Initialization
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self,x):
        return self.CNN(x)

class Regressor(nn.Module):
    """ Regressor for one characteristic, uses a fully conected stages """
    def __init__(self,input_dim,output_dim):
        super().__init__()
        # Dimensions
        self.input_dim = input_dim
        self.output_dim = output_dim
        N = 4
        dimensions = np.linspace(input_dim,output_dim,N+1,dtype = np.int32)
        # Layer secuence
        self.FC = nn.Sequential()
        for i in range(N):
            self.FC.add_module("Linear"+str(i),nn.Linear(dimensions[i],dimensions[i+1]))
            self.FC.add_module("BatchNorm"+str(i),nn.BatchNorm1d(dimensions[i+1]))
            self.FC.add_module("ReLu"+str(i),nn.ReLU())
        self.FC.add_module("Linear"+str(N),nn.Linear(dimensions[-1],output_dim))
    def forward(self,x):
        return self.FC(x)

class splitter(nn.Module):
    """
        Class to joint Stages and regress
        temperature or deformation from signal
    """

    def __init__(self,  Ls = [1,2000,400] , output_dims = [100,500):
        super(splitter, self).__init__()

        # Signal components
        self.Ls         = Ls                            # Lenght
        self.rhos       = rhos                          # Compression rate

        # Compressor layers
        self.C_ss = Compressor(Ls[0],output_dims[0])
        self.C_cc = Compressor(Ls[1],output_dims[0]))
        self.C_ac = Compressor(Ls[2],output_dims[0]))

        # Convolutional layer
        self.CNN = Convolutional(3,1,output_dims[1])

        # Regressor layer
        self.R   = Regressor( output_dims[1] , 1)

    def forward(self, x):
        """
        X components:

                X[0:0]       || X[0           :Ls[0]]             || -> Spectral shift
                X[1:2001]    || X[Ls[0]       :Ls[0]+Ls[1]]       || -> Cross correlation
                X[2001:2401] || X[Ls[0]+Ls[1] :Ls[0]+Ls[1]+Ls[2]] || -> Autocorrelation

        """

        # Compressor layers
        bs = x.shape[0] # Batch size = x[bs,:] 
        Ls = self.Ls

        x1 = x[: ,             :Ls[0]       ].reshape(bs,-1)
        x2 = x[: ,  Ls[0]      :Ls[0]+Ls[1] ].reshape(bs,-1)
        x3 = x[: , -Ls[2]      :            ].reshape(bs,-1)

        y1 = self.C_ss(x1)
        y2 = self.C_cc(x2)
        y3 = self.C_ac(x3)

    
        # Convolutional layer
        x = torch.cat([y1,y2,y3], dim= 1) # x[bs,3,-1]
        x = self.CNN(x)

        # Regressor layer
        out = self.R(x)

        return out

def init_weights(m):
    if isinstance(m, nn.Linear):
        torch.nn.init.xavier_uniform_(m.weight)
        m.bias.data.fill_(0.01)

